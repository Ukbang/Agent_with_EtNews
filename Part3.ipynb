{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.schema import AIMessage\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "from pydantic import BaseModel, Field\n",
    "from fpdf import FPDF\n",
    "import random\n",
    "import pdfplumber\n",
    "import warnings\n",
    "import dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    query : Annotated[str, \"User Question\"]\n",
    "    answer : Annotated[str, \"LLM response\"]\n",
    "    messages : Annotated[list, add_messages]\n",
    "    tool_call : Annotated[dict, \"Tool Call Result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                 temperature=0.,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def read_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하는 도구입니다.\n",
    "    표 형식 또는 일반 텍스트가 포함된 PDF를 읽고 문자열로 반환합니다.\n",
    "    \n",
    "    file_path 예시: './files/report.pdf'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "        return text.strip() if text.strip() else \"❌ PDF에서 텍스트를 추출할 수 없습니다.\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ PDF 읽기 오류: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_pdf(content: str, filename: str = \"output.pdf\"):\n",
    "    \"\"\"\n",
    "    텍스트를 PDF 파일로 저장하는 도구입니다.\n",
    "    PDF형태의 문서로 만들어야할 때 이 도구를 사용하세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "    font_path = \"C:\\Windows\\Fonts\\MALGUN.TTF\"  # <-- 여기에 실제 폰트 파일이 있어야 함\n",
    "\n",
    "    try:\n",
    "        pdf.add_font(\"malgun\", \"\", font_path, uni=True)\n",
    "        pdf.set_font(\"malgun\", size=12)\n",
    "    except:\n",
    "        raise ValueError(\"한글 폰트가 존재하지 않습니다.\")\n",
    "    \n",
    "    for line in content.split(\"\\n\"):\n",
    "        pdf.multi_cell(0, 10, line)\n",
    "    pdf.output(f\"./files/{filename}\")\n",
    "\n",
    "    print(f\"PDF saved as ./files/{filename}\")\n",
    "\n",
    "    return {\"content\":content, \"filename\":filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [TavilySearchResults(include_domains=[\"naver.com\", \"google.com\"]), PythonAstREPLTool(), write_pdf, read_pdf, *FileManagementToolkit(root_dir=\"./files/\",\n",
    "                                                                            selected_tools=[\"file_delete\",\"list_directory\"]).get_tools()]\n",
    "search_tool, code_tool, write_tool, read_tool, delete_tool, listdir_tool= tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryChecker(BaseModel):\n",
    "    \"\"\"\n",
    "    이전의 대화 기록을 참고하여 질문에 대해 답변할 수 있는지 판단합니다.\n",
    "    답변할 수 있다면 \"yes\", 답변할 수 없다면 \"no\"를 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    yes_no : Literal[\"yes\", \"no\"] = Field(..., description=\"\"\"Use your previous conversation history to determine if you can answer your questions.\n",
    "    Return \"yes\" if you can answer, \"no\" if you can't answer.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorterm_memory(state:State):\n",
    "\n",
    "    if len(state[\"messages\"]) > 6:\n",
    "        history = state[\"messages\"][-6:-1]\n",
    "    elif len(state[\"messages\"]) == 1:\n",
    "        history = \"\"\n",
    "    else:\n",
    "        history = state[\"messages\"][:-1]\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_check(state:State):\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "                이전의 대화 기록을 참고하여 질문에 대해 답변할 수 있는지 판단합니다.\n",
    "                답변할 수 있다면 \"yes\", 답변할 수 없다면 \"no\"를 반환합니다.\n",
    "                \n",
    "                대화 기록 : {history}\n",
    "                                          \n",
    "                질문 : {query}\n",
    "                                          \n",
    "                \"\"\")\n",
    "    \n",
    "    chain = prompt | history_checker\n",
    "\n",
    "    history = shorterm_memory(state)\n",
    "\n",
    "    result = chain.invoke({\"history\":history,\n",
    "                            \"query\":state[\"query\"]})\n",
    "\n",
    "    return result.yes_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_chat(state:State):\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "                이전의 대화 기록을 참고하여 질문에 대해 답변하세요.\n",
    "                아래 대화 기록을 첨부합니다.\n",
    "                대화 기록을 통해 답변이 어렵다면 내부 지식을 참조하세요.\n",
    "                \n",
    "                대화 기록 : {history}\n",
    "                                          \n",
    "                질문 : {query}\n",
    "                                          \n",
    "                \"\"\")\n",
    "\n",
    "    \n",
    "    chain = prompt | llm\n",
    "\n",
    "    history = shorterm_memory(state)\n",
    "\n",
    "    answer = chain.invoke({\"history\":history,\n",
    "                           \"query\":state[\"query\"]})\n",
    "    \n",
    "    if len(state[\"tool_call\"]) == 0:\n",
    "        return {\"answer\":answer.content,\n",
    "                \"messages\":[answer],\n",
    "                \"tool_call\":\"사용된 기록 없음.\"}\n",
    "    else:\n",
    "        return {\"answer\":answer.content,\n",
    "                \"messages\":[answer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_checker = llm.with_structured_output(HistoryChecker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_node(state:State):\n",
    "    if len(state[\"messages\"]) == 1:\n",
    "        return {\"answer\":\"답변 없음\",\n",
    "                \"tool_call\":\"사용된 도구 없음\"}\n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(\n",
    "    state: State,\n",
    "):\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "                이전의 대화 기록을 참고하여 질문에 대해 답변하세요.\n",
    "                아래 대화 기록을 첨부합니다.\n",
    "                대화 기록을 통해 답변이 어렵다면 내부 지식을 참조하세요.\n",
    "                최근에 사용한 도구가 있다면 도구도 참고하세요. 다른 도구를 사용하는 것이 더 좋은 방법이 될 수 있습니다.\n",
    "                                                    \n",
    "                대화 기록 : {history}     \n",
    "                                          \n",
    "                최근 사용한 도구 : {tool_name}\n",
    "                                        \n",
    "                정답 : {answer}\n",
    "                                        \n",
    "                질문 : {query}\n",
    "                                          \n",
    "                \"\"\")\n",
    "\n",
    "    chain = prompt | llm_with_tools\n",
    "\n",
    "    history = shorterm_memory(state)\n",
    "\n",
    "    result = chain.invoke({\"history\" : history,\n",
    "                           \"tool_name\" : state[\"tool_call\"],\n",
    "                            \"answer\": state[\"answer\"],\n",
    "                            \"query\": state[\"query\"]})\n",
    "\n",
    "    if hasattr(result, \"tool_calls\") and len(result.tool_calls) > 0:\n",
    "        tool_calls = result.tool_calls\n",
    "        return {\"messages\": result,\n",
    "                \"tool_call\": [result]}\n",
    "    else:\n",
    "        return {\"messages\":AIMessage(content=f\"\"\"도구를 선택하지 못했습니다. 적절한 도구를 재선택하세요.\n",
    "                                        \"\"\"),\n",
    "                                    \"tool_call\":None}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerChecker(BaseModel):\n",
    "    \"\"\"\n",
    "    정답 분류기입니다.\n",
    "    \n",
    "    정답이 질문을 해결했는지 여부를 판단합니다.\n",
    "    질문을 해결하지 못했을 시 해결될 때까지 도구를 이용합니다.\n",
    "\n",
    "    질문을 해결했다면 \"end\", 해결하지 못했다면 \"tool\"을 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    end : Literal[\"end\", \"tool\"] = Field(..., description=\"\"\"You are the answer sorter.\n",
    "\n",
    "                                                                Determine if the correct answer has solved the question.\n",
    "                                                                If the question is not resolved, use the tool until it is resolved.\n",
    "\n",
    "                                                                Return \"end\" if you solved the question, or \"tool\" if you didn't.\"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_checker = llm.with_structured_output(AnswerChecker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(state:State):\n",
    "\n",
    "    return {\"answer\":state[\"messages\"][-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_check(state:State):\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    당신은 정답 분류기 어시스턴트입니다.\n",
    "    \n",
    "    정답이 질문을 해결할 수 있는지 여부를 판단합니다.\n",
    "    질문을 해결할 수 없다면 도구를 이용합니다.\n",
    "\n",
    "    질문을 해결할 수 있다면 \"end\", 아니라면 \"tool\"을 반환합니다.\n",
    "                                          \n",
    "    기존 History도 참고하여 답변하세요.\n",
    "                                          \n",
    "    History : {history}\n",
    "                            \n",
    "    정답 : {answer}\n",
    "                            \n",
    "    질문 : {query}\n",
    "    \"\"\")\n",
    "\n",
    "    chain = prompt | answer_checker\n",
    "\n",
    "    history = shorterm_memory(state)\n",
    "\n",
    "    result = chain.invoke({\"history\" : history,\n",
    "                            \"answer\": state[\"answer\"],\n",
    "                            \"query\": state[\"query\"]})\n",
    "    \n",
    "    return result.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"history_node\", history_node)\n",
    "graph_builder.add_node(\"memory_chat\", memory_chat)\n",
    "graph_builder.add_node(\"select\", select)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_node(\"response\", response)\n",
    "\n",
    "\n",
    "graph_builder.add_edge(START, \"history_node\")\n",
    "graph_builder.add_conditional_edges(\"history_node\",\n",
    "                            history_check,\n",
    "                            {\"yes\":\"memory_chat\",\n",
    "                             \"no\":\"select\"})\n",
    "graph_builder.add_edge(\"select\", \"tools\")\n",
    "graph_builder.add_edge(\"tools\", \"response\")\n",
    "graph_builder.add_edge(\"memory_chat\", \"response\")\n",
    "graph_builder.add_conditional_edges(\"response\",\n",
    "                                    answer_check,\n",
    "                                    {\"end\":END,\n",
    "                                    \"tool\":\"select\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_config(limit=20):\n",
    "\n",
    "    thread_id=random.randint(1,999999)\n",
    "\n",
    "    config = RunnableConfig(recursion_limit=limit, configurable={\"thread_id\": thread_id})\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = reset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming(query, config):\n",
    "\n",
    "    result = graph.stream({\"messages\":(\"user\", query),\n",
    "                         \"query\":query}, config=config)\n",
    "    for step in result:\n",
    "        for k, v in step.items():\n",
    "            print(f\"\\n\\n=== {k} ===\\n\\n\")\n",
    "            print(v)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = reset_config()\n",
    "\n",
    "query = \"1+1은 뭔가요?\"\n",
    "\n",
    "streaming(query, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"테슬라에 대해 조사해서 레포트 작성해서 pdf형태로 저장해주세요.\"\n",
    "\n",
    "streaming(query, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"방금 조사한 tesla_report.pdf파일 한글로 번역해서 다시 써줘\"\n",
    "\n",
    "streaming(query, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"방금 번역한 내용을 테슬라_레포트.pdf파일로 저장해줘. 루트는 './files/'에 저장해주면 돼.\"\n",
    "\n",
    "streaming(query, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = reset_config()\n",
    "\n",
    "query = \"'tesla_report.pdf' 삭제해줘\"\n",
    "\n",
    "streaming(query, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = reset_config()\n",
    "\n",
    "streaming(\"pda1.pdf파일 읽어줘\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"개강하는 날짜는 언제인가요?\"\n",
    "\n",
    "streaming(query, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"강의는 어디서 들을 수 있나요?\"\n",
    "\n",
    "streaming(query, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = reset_config()\n",
    "\n",
    "code = \"\"\"\n",
    "아래 코드 실행시켜주세요.\n",
    "\n",
    "```python\n",
    "\n",
    "result = 0\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"{i+1}번째 출력: \", i)\n",
    "    result += i\n",
    "\n",
    "print(\"최종 결과: \", result)\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "streaming(code, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = reset_config()\n",
    "\n",
    "streaming(\"\"\"모두의연구소는 어떤 곳이야?\n",
    "          깔끔하게 정리해서 레포트로 만들어줘.\n",
    "          레포트의 형식은 pdf로 저장해주면 돼.\n",
    "          이름은 \"모두의연구소_레포트.pdf\"로 해줘.\"\"\", config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Modulabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
